{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "- [Introduction](#table-of-contents)\n",
    "- [Environment](#raw-data-import)\n",
    "- [Projects](#dn-project-details)\n",
    "- [Runs](#dn-run-details-per-project)\n",
    "- [Metrics](#metrics)\n",
    "- [Tasks](#tasks)\n",
    "- [Traces](#traces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Raw data import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set DN variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "DREADNODE_API_KEY = os.getenv(\"DREADNODE_API_KEY\")\n",
    "if DREADNODE_API_KEY is None:\n",
    "    raise RuntimeError(\"DREADNODE_API_KEY not set\")\n",
    "DREADNODE_API_TOKEN = os.getenv(\"DREADNODE_API_TOKEN\")\n",
    "if DREADNODE_API_TOKEN is None:\n",
    "    raise RuntimeError(\"DREADNODE_API_TOKEN not set\")\n",
    "DREADNODE_SERVER_URL = os.getenv(\"DREADNODE_SERVER_URL\")\n",
    "if DREADNODE_SERVER_URL is None:\n",
    "    raise RuntimeError(\"DREADNODE_SERVER_URL not set\")\n",
    "\n",
    "PROJECT = \"AIRTBench\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DN project details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dreadnode\n",
    "\n",
    "# Configure the client\n",
    "dreadnode.configure(\n",
    "    token=os.getenv(\"DREADNODE_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "# Get the API client\n",
    "api = dreadnode.api()\n",
    "\n",
    "projects = api.list_projects()\n",
    "print(f\"Found {len(projects)} projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DN runs per-project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = api.get_project(PROJECT)\n",
    "runs = api.list_runs(PROJECT)\n",
    "\n",
    "print(f\"Total runs found: {len(runs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_runs = {}\n",
    "\n",
    "for project in projects:\n",
    "    print(f\"Fetching runs for project: {project.name} ({project.key})\")\n",
    "    runs = api.strikes.list_runs(project.key)\n",
    "    project_runs[project.key] = runs\n",
    "    print(f\"  Found {len(runs)} runs\")\n",
    "\n",
    "print(\"\\n# Project Runs Summary\\n\")\n",
    "for project in projects:\n",
    "    project_key = project.key\n",
    "    runs = project_runs[project_key]\n",
    "\n",
    "    print(f\"## Project: {project.name}\")\n",
    "    print(f\"- **ID**: {project.id}\")\n",
    "    print(f\"- **Key**: {project_key}\")\n",
    "    print(f\"- **Total Runs**: {len(runs)}\")\n",
    "\n",
    "    if runs:\n",
    "        print(\"\\n### Run Status Summary:\")\n",
    "        status_counts = {}\n",
    "        for run in runs:\n",
    "            status = run.status\n",
    "            if status not in status_counts:\n",
    "                status_counts[status] = 0\n",
    "            status_counts[status] += 1\n",
    "\n",
    "        for status, count in status_counts.items():\n",
    "            print(f\"- **{status}**: {count} runs\")\n",
    "\n",
    "        print(\"\\n### Latest 5 Runs:\")\n",
    "        for i, run in enumerate(runs[:5]):\n",
    "            print(f\"- Run {i+1}: {run.id} ({run.status}) - {run.timestamp}\")\n",
    "\n",
    "    print(\"\\n---\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in runs:\n",
    "    print(run.id, run.status, run.timestamp, run.duration)\n",
    "\n",
    "print(f\"\\nFound a total of {len(runs)} runs for project {project.name} ({project.key})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter failed runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter failed runs\n",
    "runs = [run for run in runs if run.status == \"completed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optional**: Filter by run timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs = [run for run in runs if run.timestamp > datetime(2025, 2, 28, 6, 20)]\n",
    "# print(len(runs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Optional**: Filter runs by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs = [run for run in runs if run.params.get(\"model\") == \"claude-3-7-sonnet-20250219\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "`dn.log_metrics` is a function that returns a dataframe with the metrics of the runs. SDK reference [here](https://vscode.dev/github/dreadnode/callisto/blob/ads/eng-1705-initial-notebook-data-analysis-for-callistodnode/__init__.py#L15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest run\n",
    "if runs:\n",
    "    latest_run = runs[0]  # Assuming runs are sorted with the latest first\n",
    "\n",
    "    # Create a DataFrame to display all metrics\n",
    "    import pandas as pd\n",
    "    from datetime import datetime\n",
    "\n",
    "    metrics_data = []\n",
    "\n",
    "    print(f\"Run ID: {latest_run.id}\")\n",
    "    print(f\"Started: {latest_run.timestamp}\")\n",
    "    print(f\"Status: {latest_run.status}\")\n",
    "    print(f\"Duration: {latest_run.duration}\")\n",
    "    print(\"\\nAvailable metrics:\")\n",
    "\n",
    "    # Print the metric names available\n",
    "    for metric_name, metric_points in latest_run.metrics.items():\n",
    "        print(f\"- {metric_name}: {len(metric_points)} data points\")\n",
    "\n",
    "        # Get the latest value for each metric\n",
    "        if metric_points:\n",
    "            last_point = metric_points[-1]\n",
    "            metrics_data.append({\n",
    "                \"Metric\": metric_name,\n",
    "                \"Last Value\": last_point.value,\n",
    "                \"Step\": last_point.step,\n",
    "                \"Timestamp\": last_point.timestamp\n",
    "            })\n",
    "\n",
    "    # Create a DataFrame with metric details\n",
    "    if metrics_data:\n",
    "        metrics_df = pd.DataFrame(metrics_data)\n",
    "        display(metrics_df)\n",
    "    else:\n",
    "        print(\"No metric data points available\")\n",
    "\n",
    "    # Print all available parameters/metadata\n",
    "    print(\"\\nRun Parameters:\")\n",
    "    params_df = pd.DataFrame([latest_run.params]).T.reset_index()\n",
    "    params_df.columns = [\"Parameter\", \"Value\"]\n",
    "    display(params_df)\n",
    "else:\n",
    "    print(\"No runs found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and export a dataframe with the metrics of the runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics_dataframe_for_current_project(runs, project_key):\n",
    "    \"\"\"\n",
    "    Create a comprehensive DataFrame of metrics from filtered runs for a specific project.\n",
    "\n",
    "    Args:\n",
    "        runs: List of already filtered run objects\n",
    "        project_key: Project key these runs belong to\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: DataFrame containing all metrics with run metadata\n",
    "    \"\"\"\n",
    "    all_metrics = []\n",
    "    all_scores = []\n",
    "\n",
    "    for run in runs:\n",
    "        # Base metadata for each run\n",
    "        run_metadata = {\n",
    "            \"project_key\": project_key,\n",
    "            \"run_id\": str(run.id),\n",
    "            \"run_name\": run.name,\n",
    "            \"timestamp\": run.timestamp,\n",
    "            \"status\": run.status,\n",
    "            \"duration\": run.duration,\n",
    "            \"tags\": \", \".join(run.tags) if run.tags else \"\"\n",
    "        }\n",
    "\n",
    "        # Extract parameters\n",
    "        params = {}\n",
    "        for param_name, param_value in run.params.items():\n",
    "            params[f\"param_{param_name}\"] = param_value\n",
    "\n",
    "        # Process each metric\n",
    "        for metric_name, metric_points in run.metrics.items():\n",
    "            for point in metric_points:\n",
    "                metric_data = {\n",
    "                    **run_metadata,\n",
    "                    **params,\n",
    "                    \"data_type\": \"metric\",\n",
    "                    \"metric_name\": metric_name,\n",
    "                    \"value\": point.value,\n",
    "                    \"step\": point.step,\n",
    "                    \"metric_timestamp\": point.timestamp\n",
    "                }\n",
    "                all_metrics.append(metric_data)\n",
    "\n",
    "        # Process scores if available\n",
    "        if hasattr(run, 'scores') and run.scores:\n",
    "            for score in run.scores:\n",
    "                score_data = {\n",
    "                    **run_metadata,\n",
    "                    **params,\n",
    "                    \"data_type\": \"score\",\n",
    "                    \"metric_name\": score.name,\n",
    "                    \"value\": score.value,\n",
    "                    \"step\": 0,  # Scores typically don't have steps\n",
    "                    \"metric_timestamp\": run.timestamp,  # Use run timestamp\n",
    "                }\n",
    "\n",
    "                # Add score attributes as additional columns\n",
    "                if hasattr(score, 'attributes') and score.attributes:\n",
    "                    for attr_name, attr_value in score.attributes.items():\n",
    "                        score_data[f\"score_{attr_name}\"] = attr_value\n",
    "\n",
    "                all_scores.append(score_data)\n",
    "\n",
    "    # Combine metrics and scores\n",
    "    all_data = all_metrics + all_scores\n",
    "\n",
    "    # Create DataFrame\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(all_data)\n",
    "\n",
    "    # Add computed columns\n",
    "    if not df.empty:\n",
    "        # Calculate time from run start to metric\n",
    "        if \"timestamp\" in df.columns and \"metric_timestamp\" in df.columns:\n",
    "            df[\"time_from_start\"] = (df[\"metric_timestamp\"] - df[\"timestamp\"]).dt.total_seconds()\n",
    "\n",
    "        # Create a flag_found column for easier filtering\n",
    "        if \"metric_name\" in df.columns:\n",
    "            df[\"is_flag\"] = df[\"metric_name\"].isin([\"found_flag\", \"flag_found\"])\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use the already filtered runs (which should be filtered for completed status)\n",
    "project_metrics_df = create_metrics_dataframe_for_current_project(runs, PROJECT)\n",
    "\n",
    "# Export to CSV and Parquet with the project name in the filename\n",
    "csv_path = os.path.join(datasets_dir, f\"{PROJECT}_metrics.csv\")\n",
    "parquet_path = os.path.join(datasets_dir, f\"{PROJECT}_metrics.parquet\")\n",
    "\n",
    "project_metrics_df.to_csv(csv_path, index=False)\n",
    "project_metrics_df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"Created DataFrame with {len(project_metrics_df)} data points from {len(runs)} filtered runs\")\n",
    "print(f\"Files saved to: {datasets_dir}\")\n",
    "if not project_metrics_df.empty:\n",
    "    print(f\"Unique metrics: {project_metrics_df['metric_name'].unique()}\")\n",
    "    print(f\"Data types: {project_metrics_df['data_type'].unique()}\")\n",
    "    print(f\"Time span: {project_metrics_df['timestamp'].min()} to {project_metrics_df['timestamp'].max()}\")\n",
    "\n",
    "    # Show flag-related data if available\n",
    "    flag_data = project_metrics_df[project_metrics_df[\"is_flag\"] == True]\n",
    "    if not flag_data.empty:\n",
    "        print(f\"\\nFound {len(flag_data)} flag events across {flag_data['run_id'].nunique()} runs\")\n",
    "else:\n",
    "    print(\"No data found in the filtered runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep dive into the metrics dataframe flags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the code in in the strikes SDK `main.py`, it appears that while the flag values should theoretically be in the scores' attributes, they're truncated or only partially stored:\n",
    "\n",
    "```python\n",
    "flag_score = dn.Score(\n",
    "    name=\"flag_found\",\n",
    "    value=1.0,\n",
    "    attributes={\n",
    "        \"challenge_id\": challenge.id,\n",
    "        \"flag\": match[:10] + \"...\",  # Only first 10 chars are stored\n",
    "    },\n",
    ")\n",
    "```\n",
    "\n",
    "So while our dataframe is capturing everything available from the API, the full flag values aren't included in the data returned from the API itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n### In-depth Flag Analysis\")\n",
    "\n",
    "if not project_metrics_df.empty and 'is_flag' in project_metrics_df.columns:\n",
    "    flag_runs = project_metrics_df[project_metrics_df['is_flag'] == True]['run_id'].unique()\n",
    "\n",
    "    if len(flag_runs) > 0:\n",
    "        print(f\"Found {len(flag_runs)} runs with flags\")\n",
    "\n",
    "        # Direct inspection of flag data\n",
    "        flag_metrics = project_metrics_df[project_metrics_df['is_flag'] == True]\n",
    "        flag_summary = flag_metrics.groupby(['run_id', 'metric_name']).agg({\n",
    "            'value': 'sum',\n",
    "            'step': 'max',\n",
    "            'param_challenge': 'first',\n",
    "            'param_model': 'first',\n",
    "            'tags': 'first'\n",
    "        }).reset_index()\n",
    "\n",
    "        # Show flag distribution by challenge\n",
    "        challenge_counts = flag_summary.groupby('param_challenge')['value'].sum().sort_values(ascending=False)\n",
    "\n",
    "        print(\"\\nFlags found by challenge:\")\n",
    "        print(challenge_counts)\n",
    "\n",
    "        # Show flag distribution by model\n",
    "        if 'param_model' in flag_summary.columns:\n",
    "            model_counts = flag_summary.groupby('param_model')['value'].sum().sort_values(ascending=False)\n",
    "            print(\"\\nFlags found by model:\")\n",
    "            print(model_counts)\n",
    "\n",
    "        # Get the top 5 runs with the most flags\n",
    "        top_flag_runs = flag_summary.groupby('run_id')['value'].sum().sort_values(ascending=False).head(5)\n",
    "        print(\"\\nTop 5 runs with most flags:\")\n",
    "        print(top_flag_runs)\n",
    "\n",
    "        # Show sample flag data for analysis\n",
    "        print(\"\\nSample flag events (first 10):\")\n",
    "        display(flag_metrics[['run_id', 'metric_name', 'value', 'param_challenge', 'timestamp']].head(10))\n",
    "\n",
    "        # Try to extract flag patterns from run tags\n",
    "        if 'tags' in flag_summary.columns:\n",
    "            print(\"\\nTags from flag-successful runs:\")\n",
    "            tag_list = flag_summary['tags'].str.split(', ').explode().dropna().unique()\n",
    "            if len(tag_list) > 0:\n",
    "                for tag in tag_list[:20]:  # Show up to 20 unique tags\n",
    "                    print(f\"- {tag}\")\n",
    "\n",
    "        # Based on the attached notebook, we might see patterns by grouping\n",
    "        print(\"\\nFlag success rate by challenge:\")\n",
    "        if 'is_flag' in project_metrics_df.columns and 'param_challenge' in project_metrics_df.columns:\n",
    "            # Group flags by challenge\n",
    "            challenge_success = project_metrics_df[project_metrics_df['is_flag'] == True].groupby('param_challenge')['value'].sum()\n",
    "            # Count total run attempts by challenge\n",
    "            challenge_attempts = project_metrics_df.groupby('param_challenge')['run_id'].nunique()\n",
    "\n",
    "            # Combine into a success rate table\n",
    "            challenge_stats = pd.DataFrame({\n",
    "                'flags_found': challenge_success,\n",
    "                'run_attempts': challenge_attempts\n",
    "            }).fillna(0)\n",
    "            challenge_stats['success_rate'] = challenge_stats['flags_found'] / challenge_stats['run_attempts']\n",
    "            challenge_stats = challenge_stats.sort_values('success_rate', ascending=False)\n",
    "\n",
    "            display(challenge_stats)\n",
    "    else:\n",
    "        print(\"No runs with flags found.\")\n",
    "else:\n",
    "    print(\"No flag information available in the DataFrame.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
